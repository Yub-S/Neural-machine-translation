{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5LycdZ2p4YG",
        "outputId": "f3983e36-fd6e-401d-e250-d9b15387671d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/train.en.en\") as f:\n",
        "  english_text = f.read().split(\"\\n\")[:-2]\n",
        "\n",
        "with open('/content/drive/MyDrive/train.ne') as f:\n",
        "  nepali_text = f.read().split('\\n')[:-2]\n",
        "\n",
        "len(english_text),len(nepali_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHGdgijbp-e7",
        "outputId": "b545264a-83ec-4cb0-e42e-2650f4f683d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1780247, 1780247)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentences = []\n",
        "for sentence in english_text:\n",
        "  english_sentences.append(sentence.lower())"
      ],
      "metadata": {
        "id": "O5A9n5EtqIRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "tokenizer = Tokenizer(BPE(unk_token='[unk]'))\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(vocab_size = 50000,special_tokens=['[pad]','[unk]','[start]','[end]'])"
      ],
      "metadata": {
        "id": "AqVE6ZORqUrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('english_sentences.txt','w',encoding = 'utf-8')as f:\n",
        "  for sentence in english_sentences:\n",
        "    f.write(sentence+'\\n')\n",
        "\n",
        "with open('nepali_sentences.txt','w',encoding ='utf-8')as f:\n",
        "  for sentence in nepali_text:\n",
        "    f.write(sentence+'\\n')"
      ],
      "metadata": {
        "id": "795yVIMgqbMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(['english_sentences.txt'],trainer)\n",
        "tokenizer.save('/content/english_tokenizer.json')"
      ],
      "metadata": {
        "id": "1twZ7HFfqfrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(['nepali_sentences.txt'],trainer)\n",
        "tokenizer.save('/content/nepali_tokenizer.json')"
      ],
      "metadata": {
        "id": "RA3KXYTyqeED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_tokenizer = tokenizer.from_file('/content/english_tokenizer (1).json')\n",
        "target_tokenizer = tokenizer.from_file('/content/nepali_tokenizer (1).json')"
      ],
      "metadata": {
        "id": "ORPReWnpqnSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = target_tokenizer.encode(\"नेपालसँग जोडिएको राज्य जसले मोदीलाई पुनः प्रधानमन्त्री बनाउने वा नबनाउने निर्णय गर्छ |\")\n",
        "print(output.ids,target_tokenizer.decode([12502, 5913, 1462, 1763, 22607, 2610, 1797, 2257, 1020, 23711, 1737, 2519, 95]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdhKnMxZqveN",
        "outputId": "c8bc23c5-1378-4c2e-d44b-122fd795e8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12502, 5913, 1462, 1763, 22607, 2610, 1797, 2257, 1020, 23711, 1737, 2519, 95] नेपालसँग जोडिएको राज्य जसले मोदीलाई पुनः प्रधानमन्त्री बनाउने वा नबनाउने निर्णय गर्छ |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = source_tokenizer.encode('schedule for budget speech postponed by two hours')\n",
        "print(output.ids,source_tokenizer.decode([3786, 447, 2023, 3308, 6918, 505, 709, 2517]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1JYlLbqq-Oy",
        "outputId": "d99eae9b-2e3a-4f56-e63a-3b3ed5d63628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3786, 447, 2023, 3308, 6918, 505, 709, 2517] schedule for budget speech postponed by two hours\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_sen_len = [len(sentence.split(\" \") ) for sentence in english_sentences]\n",
        "nep_sen_len = [len(sentence.split(\" \") ) for sentence in nepali_text]\n",
        "import numpy as np\n",
        "np.percentile(eng_sen_len,95),np.percentile(nep_sen_len,95)\n",
        "# 95 percent of our english_sentences have length of 38 or fewer whereas 95 percent of sentences in our nepali text data have length of 30 or fewer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ1ZhbEru7oI",
        "outputId": "82e12e72-eed7-493f-8be6-b07daf7058dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38.0, 30.0)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 38\n",
        "nepali_sentences = []\n",
        "for sentence in nepali_text:\n",
        "  sentence = '[start]'+\" \"+sentence+\" \"+'[end]'\n",
        "  nepali_sentences.append(sentence)\n",
        "\n",
        "text_pairs =[]\n",
        "for i in range(len(english_sentences)):\n",
        "  if len(english_sentences[i].split(\" \")) < max_seq_len and len(nepali_sentences[i].split(\" \")) < max_seq_len:\n",
        "    text_pairs.append((english_sentences[i],nepali_sentences[i]))"
      ],
      "metadata": {
        "id": "1Vbs-2_3rHGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(text_pairs)"
      ],
      "metadata": {
        "id": "TdnqmfC6gSyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_train_pairs = int(0.90*len(text_pairs))\n",
        "train_pairs = text_pairs[:len_train_pairs]\n",
        "val_pairs = text_pairs[len_train_pairs:]"
      ],
      "metadata": {
        "id": "PX3VQjiEqjEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(train_pairs)\n",
        "random.shuffle(val_pairs)\n",
        "sample_train_pairs = train_pairs[:500000]\n",
        "sample_val_pairs = val_pairs[:30000]\n",
        "train_eng_sen,train_nep_sen = zip(*sample_train_pairs)\n",
        "val_eng_sen,val_nep_sen = zip(*sample_val_pairs)"
      ],
      "metadata": {
        "id": "RVuJwiHprUx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_eng_sen[-1],train_nep_sen[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-oUh6Snfh5y",
        "outputId": "101371dc-e6f4-4cbc-b711-2423e75b6d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('that is why he maintained open business freedom within his empire',\n",
              " '[start] त्यस कारण उसले आफ्नो साम्राज्य भित्र खुल्ला ब्यापारिक स्वतन्त्रता कायम गरेको थियो [end]')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def tokenize (sentences,max_len,value,tokenizer):\n",
        "  tokenized_sentences = []\n",
        "  for sentence in sentences:\n",
        "    output = tokenizer.encode(sentence)\n",
        "    tokenized_sentences.append(output.ids)\n",
        "\n",
        "  padded_sentences = pad_sequences(tokenized_sentences,maxlen=max_len,padding='post',truncating = 'post',value = value)\n",
        "  return padded_sentences\n",
        "\n",
        "train_en_sen =tokenize(train_eng_sen,max_seq_len,0,source_tokenizer)\n",
        "train_ne_sen = tokenize(train_nep_sen,max_seq_len+1,0,target_tokenizer)\n",
        "val_en_sen = tokenize(val_eng_sen,max_seq_len,0,source_tokenizer)\n",
        "val_ne_sen = tokenize(val_nep_sen,max_seq_len+1,0,target_tokenizer)"
      ],
      "metadata": {
        "id": "FsgaZe-vrmQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def format_dataset(eng_sen,nep_sen):\n",
        "  return ({\n",
        "      'english' :eng_sen,\n",
        "      'nepali' :nep_sen[:,:-1],\n",
        "  }),nep_sen[:,1:]\n",
        "\n",
        "def make_dataset(eng_sen,nep_sen):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_sen,nep_sen))\n",
        "  dataset = dataset.batch(64)\n",
        "  dataset = dataset.map(format_dataset,num_parallel_calls=4)\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_en_sen,train_ne_sen)\n",
        "val_ds = make_dataset(val_en_sen,val_ne_sen)"
      ],
      "metadata": {
        "id": "VpnzlPelromf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs,target in train_ds.take(1):\n",
        "  print(f\"encoder input shape:{inputs['english'].shape}\")\n",
        "  print(f\"decoder input shape: {inputs['nepali'].shape}\")\n",
        "  print(f\"target shape:{target.shape}\")\n",
        "  print(source_tokenizer.decode(inputs['english'][0]),target_tokenizer.decode(inputs['nepali'][0]))\n",
        "  print(inputs['nepali'][0],target[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZIyt0UQrqpv",
        "outputId": "1b42d3d9-a72c-4f03-d14c-b8739d2385a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder input shape:(64, 38)\n",
            "decoder input shape: (64, 38)\n",
            "target shape:(64, 38)\n",
            "liberation is a genre of literature मुक्तक साहित्यको एउटा बिधा हो\n",
            "tf.Tensor(\n",
            "[    2 34841  9215  1473 16563  1052     3     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0], shape=(38,), dtype=int32) tf.Tensor(\n",
            "[34841  9215  1473 16563  1052     3     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0], shape=(38,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model building"
      ],
      "metadata": {
        "id": "H1UrCFVZr0Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "-z-jGGXBr1f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# positional encoding\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "i4ySGaV7r3qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# positional_encoding + embeddings\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=38, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ],
      "metadata": {
        "id": "Tfw3Wch8r3m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ],
      "metadata": {
        "id": "B5IYju_hr3kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=False)\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "NNqx9OmRsAAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "-4xjnQwWr_9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "SALh8UTtr_7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ihMk2go5r_lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "zjSwCNQhsbyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    x = self.pos_embedding(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`\n"
      ],
      "metadata": {
        "id": "JNX9oFUysbp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ],
      "metadata": {
        "id": "WntGEiPLsbmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.pos_embedding(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ncBEasGKs_S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    context, x  = inputs['english'],inputs['nepali']\n",
        "\n",
        "    context = self.encoder(context)\n",
        "\n",
        "    x = self.decoder(x, context)\n",
        "\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "NuQpYSubtWdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "BH39fDputrRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=50000,\n",
        "    target_vocab_size=50000,\n",
        "    dropout_rate=dropout_rate)"
      ],
      "metadata": {
        "id": "fTR0WDybts8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "fiIrufXztwUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_objects = {'Transformer':Transformer,'masked_loss':masked_loss,'masked_accuracy':masked_accuracy}\n",
        "transformer = keras.models.load_model(\"/content/drive/MyDrive/transformer_model_latest_final.keras\",custom_objects = custom_objects)"
      ],
      "metadata": {
        "id": "N00BA1N2tzxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer='rmsprop',\n",
        "    metrics=[masked_accuracy])\n",
        "\n",
        "from tensorflow import keras\n",
        "callbacks = [keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/transformer_model_latest_final.keras',save_best_only = False,verbose = 1)]\n",
        "\n",
        "transformer.fit(train_ds,\n",
        "                epochs=10,\n",
        "                initial_epoch=8,\n",
        "                validation_data=val_ds,callbacks  = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgo_WbrIt1l9",
        "outputId": "482a8854-8fff-456f-bd36-3fadda8aa330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10\n",
            "7813/7813 [==============================] - ETA: 0s - loss: 2.9736 - masked_accuracy: 0.5564\n",
            "Epoch 9: saving model to /content/drive/MyDrive/transformer_model_latest_final.keras\n",
            "7813/7813 [==============================] - 2712s 344ms/step - loss: 2.9736 - masked_accuracy: 0.5564 - val_loss: 2.9813 - val_masked_accuracy: 0.5670\n",
            "Epoch 10/10\n",
            "4070/7813 [==============>...............] - ETA: 20:44 - loss: 2.9566 - masked_accuracy: 0.5588"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_objects = {'Transformer':Transformer,'masked_loss':masked_loss,'masked_accuracy':masked_accuracy}\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/transformer_model_latest_final.keras\",custom_objects = custom_objects)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SD1bAX6auDmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29255f3-270c-4197-fe80-61d0fc6f3bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_1 (Encoder)         multiple                  23320576  \n",
            "                                                                 \n",
            " decoder_1 (Decoder)         multiple                  31736832  \n",
            "                                                                 \n",
            " dense_33 (Dense)            multiple                  12850000  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67907408 (259.05 MB)\n",
            "Trainable params: 67907408 (259.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "nep_vocab = target_tokenizer.get_vocab()\n",
        "max_decoded_sentence_length = 38\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    # Tokenize the input sentence\n",
        "    tokenized_input_sentence = source_tokenizer.encode(input_sentence).ids\n",
        "    tokenized_input_sentence = tf.convert_to_tensor([tokenized_input_sentence], dtype=tf.int32)\n",
        "\n",
        "    # Initialize the decoded sentence\n",
        "    decoded_sentence = '[start]'\n",
        "    decoded_sentence_tokens = target_tokenizer.encode(decoded_sentence).ids\n",
        "    decoded_sentence_tokens = tf.convert_to_tensor([decoded_sentence_tokens], dtype=tf.int32)\n",
        "\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        # Create the input dictionary for the model\n",
        "        inputs = {\n",
        "            'english': tokenized_input_sentence,\n",
        "            'nepali': decoded_sentence_tokens\n",
        "        }\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = model(inputs, training=False)\n",
        "\n",
        "        # Select the last token from the `seq_len` dimension.\n",
        "        predictions = predictions[:, -1, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "        # Get the index of the highest probability token\n",
        "        sampled_token_index = tf.argmax(predictions, axis=-1).numpy().item()\n",
        "\n",
        "\n",
        "        sampled_token = target_tokenizer.decode([sampled_token_index])\n",
        "\n",
        "\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == '[end]':\n",
        "            break\n",
        "\n",
        "        decoded_sentence_tokens = target_tokenizer.encode(decoded_sentence, add_special_tokens=False).ids\n",
        "        decoded_sentence_tokens = tf.convert_to_tensor([decoded_sentence_tokens], dtype=tf.int32)\n",
        "\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "WPXblWUfuKEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"although a large number of Indian and American tourists entered the country, the number of tourists from China has not increased as per expectation.\"\n",
        "decoded_sentence = decode_sequence(sentence.lower())\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "id": "HjPxegPwuNhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1baeb7-e71b-49c6-d109-7782b8d141d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[start] ठूलो संख्यामा भारतीय र अमेरिकी पर्यटक देश भित्र पस्यो तापनि चीनबाट पर्यटकहरूको सङ्ख्या अपेक्षा अनुसार बढेको छैन ।                    \n"
          ]
        }
      ]
    }
  ]
}